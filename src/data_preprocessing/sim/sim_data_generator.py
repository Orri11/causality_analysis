import random

import numpy as np
import pandas as pd
from statsmodels.tsa.stattools import adfuller


### Implementation of the energy data synthesizer ammended from Pyraformer
class SynthesisTS:
    """
    The class is an adaptation of the original energy time series generator developed
    by the authors of Pyraformer.

    The original script was restructured into a class for better readability and
    to reflect the order of operations in the synthesis process.
    Further documentation and comments were added.
    """

    def __init__(
        self,
        cycle_periods=[1 , 7 , 30 ],
        distort_cycles=False,
        series_amount=10,
        seq_len=84,
    ):
        """
        Args:
            cycle_periods (list[int, int, int]): Cycle periods to model dependency.
                                                 For hourly data = [24, 168, 720],
                                                 which == [day, week, month].
            distort_cyles (
                list[
                    tuple(int, int),
                    tuple(int, int),
                    tuple(int, int),
                    ]
            ): Optionally distort the cycle periods to model dependencies.
            series_amount (int): How many synthetic series to generate. (default = 60).
            seq_len (int): The length of the synthetic series to be generated.
                           (default = 20 months).
        """
        self.cycle_periods = cycle_periods
        self.distort_cycles = distort_cycles
        self.series_amount = series_amount
        self.seq_len = seq_len

    # Generates the actual energy time series
    def _generate_sin(self, time_points, sin_coefficients):
        """
        Generates a mixed sinusoidal sequence given the amount of cycle periods,
        time points, and the amount of coefficients for individual sine functions.
        """
        if self.distort_cycles:
            if len(self.distort_cycles) != len(self.cycle_periods):
                raise ValueError(
                    "The lengths of cycle_periods and distort_cycles arrays do not "
                    "match."
                )
        # Empty array corresponding to time points
        y = np.full(len(time_points), 100.0)
        # Generate individual sine functions to sum up
        for i in range(len(self.cycle_periods)):
            # If distortion chosen
            if self.distort_cycles:
                # Get minimum and maximum points
                min_point, max_point = self.distort_cycles[i]
                # container
                sines = np.zeros((max_point - min_point + 1, len(time_points)))
                # Iterate over the range and generate a sine wave for each parameter
                for j, period in enumerate(range(min_point, max_point + 1)):
                    sines[j, :] = sin_coefficients[i] * np.sin(
                        2 * np.pi / period * time_points
                    )

                y += np.mean(sines, axis=0)
            else:
                y += sin_coefficients[i] * np.sin(
                    2 * np.pi / self.cycle_periods[i] * time_points
                )

        return y

    # Defines a polynomially decaying covariance of B_0
    def _polynomial_decay_cov(self):
        """
        Defines polynomially decaying covariance function for B_0, which will be then
        drawn from Gaussian distribution.
        """
        # Mean at each time point t will be 0
        mean = np.zeros(self.seq_len)
        # Obtaining distance matrix
        x_axis = np.arange(self.seq_len)
        distance = x_axis[:, None] - x_axis[None, :]
        distance = np.abs(distance)
        # Apply polynomial decay
        cov = 1 / (distance + 1)
        return mean, cov

    def _multivariate_normal(self, mean, cov):
        """
        Generates (series_amount) of Gaussian distributions for drawing the noise terms.
        Takes mean and cov generated by _polynomial_decay_cov as argument.
        """
        noise = np.random.multivariate_normal(mean, cov, (self.series_amount,), "raise")
        return noise

    def synthesize_series(self):
        """
    
        Generates a single time series with a date-time index.

        Returns:
            self.results (pd.DataFrame): univariate time series sequence stored in a
                                         Pandas DataFrame alongside a time index column.
        """
        data = []
        for i in range(self.series_amount):
            # Generate initial time stamp
            #init_date_stamp = pd.Timestamp("1990-01-01")
            # Generate fake hour within month of january
            # "Start of each time series t_0 is uniformly sampled from [0, 30]"
            start = int(np.random.uniform(0, self.cycle_periods[-1]))
            # Generate time points for the _generate_sin
            time_points = start + np.arange(self.seq_len)
            # Obtain 'real' start date of the series
            #real_start_date = init_date_stamp + pd.to_timedelta(start, "D")
            # Generate datetime index for entire sequence
            #datetime_index = pd.date_range(
                #start=real_start_date, periods=self.seq_len, freq="D")
            
            # Coefficients of the sine functions B_1, B_2, B_3 uniformly from [5, 10]
            sin_coefficients = np.random.uniform(5, 10, 3)
            # Generate time series
            y = self._generate_sin(time_points, sin_coefficients)
            data.append(y)
        data = np.array(data)
        # Define mean and covariance of the noise term B_0
        mean, cov = self._polynomial_decay_cov()
        # Draw B_0 -s for each time point t for from Gaussian distribution
        noise = self._multivariate_normal(mean, cov)
        data = data + noise
        # Format to make compatible with TransformersTS
        df = pd.DataFrame({i : data[i].squeeze() for i in range(data.shape[0])})

        # Add as a self variable
        self.result = df
        return self.result

    @staticmethod
    def add_multiplicative_trend(
        df=None,
        trend_rate=1.00005,
        reversal_offset=0.00002,
        reversal=False,
        reversal_timepoints=None,
    ):
        """
        Adds a multiplicative trend and optionally reversals to an existing time series.

        Args:
            df(pd.DataFrame, optional): DataFrame containing the synthesized time series
                                        to apply trend to. Will default to self.result
                                        if exists.
            trend_rate (float): Multiplicative trend rate.
            reversal_offset (float): A rate to offset the trend rate by to make
                                     reversals smoother.
            reversal (bool): Whether to apply trend reversals.
            reversal_timepoints (list[int]): A list of time steps t at which reversals
                                             should happen.
        """
        if df is None:
            raise ValueError("No data given: please provide a DataFrame.")
        if reversal and reversal_timepoints is None:
            raise ValueError(
                "No reversal time points given. Please provide a list[int]."
            )

        result_df = df.copy()
        
        current_rate = trend_rate
        for i in range(0, result_df.shape[1]):
            y = result_df[i].values
            # Define time points t
            t = np.arange(len(result_df[i]))
            for j in range(0, len(t)):
                # Invert trend rate
                y[j] = y[j] * ((current_rate**j))
            result_df[i] = y

        return result_df

    @staticmethod
    def add_intervention(
        df= None,
        treatment_rate= 0.3,
        length = 24,
        type = 'hom'
    ):
    
        """
        Splits the series in the dataset into control and treated units and applies the intervention effect
        to the treated units.

        Args:
            df(pd.DataFrame, optional): DataFrame containing the synthesized time series
                                        to apply the intervention to. Will use self.result if
                                        existing.
            treatment_rate (float): Proportion of units in the dataset to apply intervention to.
            length(int): Length of the intervention effect (the prediction length).
            type (str): Type of intervention effect. 'hom' for homogeneous and 'het' for heterogeneous.
        """
        if df is None:
            raise ValueError("No data given: please provide a DataFrame.")
        
        df = df.copy()
        data_len = df.shape[0]

        num_treatment = int(df.shape[1] * (treatment_rate))
        #Randomly split the dataset into control and treated units
        units_for_treatment = np.random.choice(np.arange(0, df.shape[1]), num_treatment, replace=False)
        mask = np.ones(df.shape[1], dtype =bool)
        mask[units_for_treatment] = False
        control_units = df.iloc[:, mask]
        treated_units = df.iloc[:, units_for_treatment]
        treated_units_intrv = treated_units.copy()
        #Define the quantiles and multipliers for the intervention
        quantile_list = np.linspace(0, 0.8, 5)
        multiplier_list = np.linspace(0.3, 1.5, 5)
        quantile_dict = {}
        multiplier_dict = {}
        for index, quantile in enumerate(quantile_list):
            #Extract the values of the quantiles for the treated units
            quantile_dict[quantile] = np.quantile(treated_units, quantile)
            #Create a dictionary of multipliers for each quantile (from 0.2 std to 1 std)
            multiplier_dict[quantile] = multiplier_list[index]

        #Calculate the standard deviation of the treated units prior to the intervention
        std_treated = treated_units.iloc[:data_len-length,:].values.std()
        #Apply the intervention effect
        if type == 'hom':
            for i in range (len(quantile_list)):
                if i < (len(quantile_list) - 1 ):
                    for j in range(treated_units.shape[1]):
                        values = treated_units.iloc[data_len-length:, j].loc[(treated_units.iloc[data_len-length:,j] >= 
                        quantile_dict[quantile_list[i]]) & 
                        (treated_units.iloc[data_len-length:, j] < quantile_dict[quantile_list[i+1]])]   
                        treated_units_intrv.iloc[values.index.values, j] -= std_treated * multiplier_dict[quantile_list[i]]
                else:
                    for j in range(treated_units.shape[1]):
                        values = treated_units.iloc[data_len-length:,j].loc[treated_units.iloc[data_len-length:,j] >= 
                        quantile_dict[quantile_list[i]]]
                        treated_units_intrv.iloc[values.index.values,j] -= std_treated * multiplier_dict[quantile_list[i]] 
        elif type == 'het':
            for i in range (len(quantile_list)):
                if i < (len(quantile_list) - 1 ):
                    for j in range(treated_units.shape[1]):
                        values = treated_units.iloc[data_len-length:, j].loc[(treated_units.iloc[data_len-length:,j] >= 
                        quantile_dict[quantile_list[i]]) & 
                        (treated_units.iloc[data_len-length:, j] < quantile_dict[quantile_list[i+1]])]
                        treated_units_intrv.iloc[values.index.values, j] -= \
                        std_treated * ( multiplier_dict[quantile_list[i]] +  np.random.uniform(-0.1,0.1))
                else:
                    for j in range(treated_units.shape[1]):
                        values = treated_units.iloc[data_len-length:,j].loc[treated_units.iloc[data_len-length:,j] >= 
                        quantile_dict[quantile_list[i]]]
                        treated_units_intrv.iloc[values.index.values,j] -= \
                        std_treated * ( multiplier_dict[quantile_list[i]] +  np.random.uniform(-0.1,0.1))

        result_df = pd.concat([control_units, treated_units_intrv], axis=1)
        result_df = result_df[df.columns] #restore original order of columns
        return result_df, units_for_treatment

    @staticmethod
    def add_additive_trend(
        df=None,
        trend_slope=0.01,
        trend_slope_increment=0.005,
        accumulated_retain_rate=0.5,
        reversal=False,
        reversal_timepoints=None,
    ):
        """
        Adds an additive trend and optionally reversals to an existing time series.

        Args:
            df(pd.DataFrame, optional): DataFrame containing the synthesized time series
                                        to apply trend to. Will use self.result if
                                        existing.
            trend_slope (float): Additive trend slope.
            trend_slope_increment (float): Increase trend_rate when coming up from a
                                           decreasing trend --- helps maintain overall
                                           rising trend.
            accumulated_retain_rate (float): How much of the accumulated trend to retain
                                             -- helps make additive trend reversals
                                             smoother
            reversal (bool): Whether to apply trend reversals.
            reversal_timepoints (list[int]): A list of time steps t at which reversals
                                             should happen.
        """
        # If dataframe is not given use self.result
        if df is None:
            raise ValueError("No data given: please provide a DataFrame.")

        if reversal and reversal_timepoints is None:
            raise ValueError(
                "No reversal time points given. Please provide a list[int]."
            )

        df = df.copy()

        # Define series y
        y = df["TARGET"].values
        # Define time points t
        t = np.arange(len(y))

        # Variables to handle trend reversals more smoothly
        accumulated = 0
        from_increase_path = True

        for i in range(0, len(t)):
            if reversal and i in reversal_timepoints:
                # Just reverse direction if trend additive
                trend_slope = -trend_slope
                # Increment and from_increase_path allow us to
                # increment trend slope when reversing from a decrease
                # Otherwise it may not catch up
                if not from_increase_path:
                    trend_slope += trend_slope_increment
                    from_increase_path = True
                else:
                    from_increase_path = False
                # Reset some part of cumulation for trend not to catch up too rapidly
                accumulated *= accumulated_retain_rate

            accumulated += trend_slope

            y[i] += accumulated

        result_df = pd.DataFrame({"TARGET": y.squeeze()}, index=df.index)

        return result_df


def describe_and_test(time_series):
    """
    Prints descriptive statistics and conducts Augmented Dickey-Fuller unit-root test on
    given time series.

    Args:
        time_series (pd.Series, pd.DataFrame): Time series sequence given as either
                                               pandas.Series or single-column
                                               pandas.DataFrame.
    """
    test_result = adfuller(time_series)
    print("=" * 60)
    print("DESCRIPTIVE STATISTICS:")
    print("-" * 60)
    print(time_series.describe())
    print("=" * 60)
    print("ADF TEST RESULTS:")
    print("-" * 60)
    print(f"Test Statistic:  {test_result[0]:.4f}")
    print(f"p-value:  {test_result[1]:.4f}")
    if test_result[1] < 0.05:
        print("Null hypothesis of non-stationarity can be rejected.")
    else:
        print("Null hypothesis of non-stationarity cannot be rejected.")
    print("=" * 60)
